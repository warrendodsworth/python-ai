{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whats your name? Short answer: It depends. I like the name, but the problem is I'd rather use my old nickname, Wanda, than the \"Saw\" it's just the way it's pronounced.\n",
      "\n",
      "\n",
      "A few people have said it sounded better from the inside out than from the side. This is the most obvious case: the \"Bun\" (a German for \"bun-beihn\"). In Japanese, the B sound gets stuck around the middle. It's the B that's the main difference: A lot of people are using this pronunciation with the \"C\" in there.\n",
      "\n",
      "\n",
      "I'm not trying to tell you that 'bun-beihn' is better. It's more like'saw', so\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.backends.mps.is_available() else -1\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    nlp_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "    )\n",
    "    return nlp_pipeline\n",
    "\n",
    "\n",
    "model_pipeline = load_model()\n",
    "prompt = f\"Whats your name? Short answer\"\n",
    "response = model_pipeline(prompt, max_new_tokens=150, num_return_sequences=1)[0][\n",
    "    \"generated_text\"\n",
    "]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo HuggingFace Inference API (serverless)\n",
    "import requests\n",
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/speecht5_tts\"\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "audio_bytes = query(\n",
    "    {\n",
    "        \"inputs\": \"The answer to the universe is 42\",\n",
    "    }\n",
    ")\n",
    "# You can access the audio with IPython.display for example\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(audio_bytes)\n",
    "print(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
